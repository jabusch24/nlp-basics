{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e1-c1-parsing_pptx challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Challenge Introduction\n",
    "This particular notebook deals with the challenge of \"parsing pptx\" (pptx = powerpoint presentations).<br>\n",
    "<br>\n",
    "**INTRO:**<br>\n",
    "PowerPoint presentations are less structured than, say, a CV or an excel sheet, however, there is more structure to be found than in plain text. Whereas in plain text we follow the paradigm \"know the meaning of a word by the company it keeps\", the same approach is hardly applicable to powerpoints. Powerpoint presentations usually follow the rather broad structure of presentation > slides > title, subtitle, paragraph. In a powerpoint, it can easily happen that a word only appears once, though it has a significant relevance to the content presented. Those words would be expected to be in the title of a slide, a first hint to a possible solution.<br>\n",
    "**INPUT:**<br>\n",
    "Data type: ppt Presentation (special class type of the python-pptx library)<br>\n",
    "Example: <class 'pptx.presentation.Presentation'><br>\n",
    "Essentially your component receives a ppt Presentation object which holds a lot of meta data on the presentation which we'ld like to make use of such as \"shape type\" and slide numbers. <br>\n",
    "**OUTPUT:** <br>\n",
    "Data type: dict/list<br>\n",
    "Example: {\"name\":\"Frenz Josef Freud\"}<br> \n",
    "The goal is to output structured data in form of dictionaries or lists e. g. {\"skills\": [\"strategic management\",\"product supply\"]} AND ALSO a string corpus which we'll sent through our plain text pipeline. Ideally this way we can capture not just the frequency-based relevance but also the position-based relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from pptx import Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Loading Input Data\n",
    "We use as input data a pretty random and publically available powerpoint. It is simple, not atypical and contains many content types we would like to cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = './presentation.pptx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function goes through the pages of the pptx document and collects the input as a string with python-pptx.\n",
    "# As this would mean we would loose ALL of the meta information from the powerpoint, this shall be the worst case scenario approach\n",
    "def pptxparsing(file):\n",
    "    # Extract text from ppt as a single string\n",
    "    str_text = \"\"\n",
    "    ppt = Presentation(file)\n",
    "    for slide in ppt.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame:\n",
    "                continue\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                for run in paragraph.runs:\n",
    "                    str_text = str_text + str(run.text) + \". \"\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pptx parsing function on our input document.\n",
    "with open(input_file_path, 'rb') as file:\n",
    "    input_string = pptxparsing(file)\n",
    "input_string\n",
    "# As we see, just plain very unstructured, noisy text which is far from ideal for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the powerpoint document (\"with open\")\n",
    "with open(input_file_path, 'rb') as file:\n",
    "    # initialize dictionary to store the strings in\n",
    "    texts = {\"notes\":[],\"titles\":[],\"paragraphs\":[]}\n",
    "    # send powerpoint document through the parser\n",
    "    pptx = Presentation(file)\n",
    "\n",
    "    # iterate through powerpoint slides\n",
    "    for slide in pptx.slides:\n",
    "        # if slide has a notes section and it's NOT empty, extract the text into dictionary\n",
    "        if slide.has_notes_slide == True and slide.notes_slide.notes_text_frame.text != \"\":\n",
    "            # ...and remove the html left overs (\\n,\\t)\n",
    "            texts[\"notes\"].append(slide.notes_slide.notes_text_frame.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "        # if the slide has a title, and it's not empty, extract title into dictionary\n",
    "        if slide.shapes.title and slide.shapes.title.text != \"\":\n",
    "            texts[\"titles\"].append(slide.shapes.title.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\"))\n",
    "\n",
    "        # shapes are the next hierarchical level under slides. They can represent textframes, pictures, rectangles etc.\n",
    "        # For the beginning we'll only look at the text frame: \n",
    "        for shape in slide.shapes:\n",
    "            # make sure that shape has text\n",
    "            if shape.has_text_frame and shape.text != \"\":\n",
    "                # extract paragraph text into dictionary\n",
    "                texts[\"paragraphs\"].append(shape.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\"))\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wonderful. Now we have all titles, paragraphs and notes in one dictionary. Whatever happens now is up to you. \n",
    "# Some ideas: \n",
    "# - Make use of spacy's named entity recognizer and extract the category (see below). \n",
    "# - Run an LDA (https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24,\n",
    "# and https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) on the titles, paragraphs and notes.\n",
    "# - Find an appropriate weighting of notes vs titles vs paragraph texts.\n",
    "# - Your ideas...\n",
    "\n",
    "# Ideally, we end up with a list or dictionary that provides us with the skill terms ranked by relevance (how to rank is up to you).\n",
    "\n",
    "# spacy's NER on the titles:\n",
    "for title in texts[\"titles\"]:\n",
    "    doc = nlp(title)\n",
    "    for ent in doc.ents:\n",
    "        if ent.end_char-ent.start_char > 3:\n",
    "            print(ent.text, \",\" ,  ent.label_)\n",
    "# if you're not familiar with spacy's named entity labeling: Spacy is pretty awesome, \n",
    "# because it can identify \"named entities\", which are essentially, *not normal words* like \"Berlin, \n",
    "# Barack Obama, or Bayer\". The ent.label_ provides us with an indication of what it is, like Berlin = GPE.\n",
    "# spacy.explain() can explain what that means:\n",
    "print(spacy.explain(\"GPE\"))\n",
    "# correct :) \n",
    "\n",
    "print(\"\\nBe aware that spacy is great, but not perfect! Many entities, especially if there is no context, will be falsely classified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy Coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
