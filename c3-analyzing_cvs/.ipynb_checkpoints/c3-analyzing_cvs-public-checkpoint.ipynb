{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c3-analyzing_cvs challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Challenge Introduction\n",
    "This particular notebook deals with the challenge of \"analyzing CVs\".<br>\n",
    "<br>\n",
    "**INTRO:**<br>\n",
    "CVs represent a special challenge for extracting data as they usually consist of *semi-structured* data, meaning, there is a structure, but it's not standardized or easily readible as in a relational database. Additionally, the terms found in the CV are pretty much on point, i.e. when a CV says \"Languages: English, Cantonese\", then that is something worth extracting as opposed to free text data where relevance is represented by occurences. As an example, remember Seamus Finnigan? No? Well...Remember Harry Potter? Same book. The assumptions of traditional NLP algorithms don't apply to semi-structured text, thus the challenge here: Extract the information with an algorithm, specifically tailored to CVs.<br>\n",
    "**INPUT:**<br>\n",
    "Data type: string<br>\n",
    "Example: \"\\n \\n \\nName\\n \\nD\\nr. J\\nohnny Depp\\n \\nAddress\\n \\nBroadway 10\"<br>\n",
    "Essentially your component receives unfiltered, human readable text and outputs in the ideal case a structured data format like a list or a dictionary of e.g. skill or previous worked companies. As pdf and docx always come in slightly different formats and version, you can expect a fair amount of noise. <br>\n",
    "**OUTPUT:** <br>\n",
    "Data type: dict/list<br>\n",
    "Example: {\"name\":\"Johnny Depp\"}<br> \n",
    "The goal is to output structured data in form of dictionaries or lists. In the worst case, you might as well output a slightly better string that then goes through the same skill extraction process as all other normal texts (worst case scenario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Loading Input Data\n",
    "We use as input data Jesus' CV. It is simple and probably a good average CV of an employee.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = './cv.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function goes through the pages of the pdf document and collects the input as a string with PyPDF2:\n",
    "def pdfparsing(file):\n",
    "    str_text = \"\"\n",
    "    pdfReader = PyPDF2.PdfFileReader(file)\n",
    "    for page in pdfReader.pages:\n",
    "        str_text = str_text + str(page.extractText()) + \". \"\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pdf parsing function on our input document\n",
    "with open(input_file_path, 'rb') as file:\n",
    "    input_string = pdfparsing(file)\n",
    "input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the input string contains a number of html \\n tags \n",
    "# which we would like to get rid of in order to effectively extract information.\n",
    "# So let's get rid of them:\n",
    "input_string.replace(\"\\n\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much better, though we still see a lot of unclean data (referring to the relevance of challenge 2 \"text cleaning\")\n",
    "# but lets work with what we have. First, where do we want to get to? \n",
    "# In the most ideal case, we reach a dictonary with {\"skill area\": \"skill\"} pairs, like {\"language\":[\"german\",\"spanish\"]}\n",
    "# or {\"jobs\":[\"scientist\",\"freelancer\",\"founder\"]}\n",
    "\n",
    "# just explorative, let's check what spacy can give us:\n",
    "doc = nlp(input_string)\n",
    "for ent in doc.ents:\n",
    "    if ent.end_char-ent.start_char > 3:\n",
    "        print(ent.text, \",\", ent.start_char,\",\", ent.end_char,\",\",  ent.label_)\n",
    "# oh right, you might not be familiar with spacy's named entity labeling. Spacy is pretty awesome, \n",
    "# because it can identify \"named entities\", which are essentially, *not normal words* like \"Berlin, \n",
    "# Barack Obama, or Bayer\". The ent.label_ provides us with an indication of what it is, like Berlin = GPE.\n",
    "# spacy.explain() can explain what that means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy.explain(\"GPE\"))\n",
    "# correct :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are multiple ways of operating:\n",
    "1. Rely on the accuracy of spaCy's named entity recognizer and just put these information into the dictionaries based on the ENT labels. (easy, quick to implement, not recommended though, because not very accurate)\n",
    "2. Create vocabulary of words we could be interested in and apply simple string matching, e.g.in pseudo code:\n",
    "interesting_words = {\"language\":[\"german\",\"english\",\"spanish\"]}\n",
    "if word in interesting_words[\"language\"]:\n",
    "    add to skills_list\n",
    "3. Nr. 2 is very simple and can be very powerful, however that solely depends on the size of our \"interesting_words\" list and probably we will never be able to cover all potentially important words. Thus a hybrid approach of 1 and 2 is probably best. \n",
    "4. Do you have any further ideas? Looking forward to see your approaches!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
